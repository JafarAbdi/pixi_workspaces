[project]
authors = ["JafarAbdi <jafar.uruc@gmail.com>"]
channels = ["conda-forge"]
description = "Add a short description here"
name = "llama_ws"
platforms = ["linux-64"]
version = "0.1.0"

[activation.env]
CMAKE_INSTALL_MODE = "SYMLINK"
CMAKE_INSTALL_PREFIX = "$CONDA_PREFIX"
GGML_CUDA_ENABLE_UNIFIED_MEMORY = "1"

[tasks]
configure = { cmd = [
  "cmake",
  "-GNinja",
  "-DCMAKE_EXPORT_COMPILE_COMMANDS=ON",
  "-DCMAKE_BUILD_TYPE=Release",
  "-DLLAMA_CURL=ON",
  "-DCURL_LIBRARY=$CONDA_PREFIX/lib/libcurl.so",
  "-DCURL_INCLUDE_DIR=$CONDA_PREFIX/include",
  "-DCURL_NO_CURL_CMAKE=ON",
  "-DGGML_CUDA=ON",
  "-S",
  "llama.cpp/",
  "-B",
  ".build/",
] }
build = { cmd = "cmake --build .build/", depends_on = ["configure"] }
install = { cmd = "cmake --install .build/ --prefix $CONDA_PREFIX", depends_on = ["build"] }

[dependencies]
libcurl = ">=8.11.1,<9"
cuda = "12.4.1"
huggingface_hub = ">=0.26.5,<0.27"
