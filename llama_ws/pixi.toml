[project]
authors = ["JafarAbdi <jafar.uruc@gmail.com>"]
channels = ["conda-forge"]
description = "Add a short description here"
name = "llama_ws"
platforms = ["linux-64"]
version = "0.1.0"

[activation.env]
CMAKE_INSTALL_MODE = "SYMLINK"
CMAKE_INSTALL_PREFIX = "$CONDA_PREFIX"
GGML_CUDA_ENABLE_UNIFIED_MEMORY = "1"

[tasks]
configure = { cmd = [
  "cmake",
  "-GNinja",
  "-DCMAKE_EXPORT_COMPILE_COMMANDS=ON",
  "-DCMAKE_BUILD_TYPE=Release",
  "-DLLAMA_CURL=ON",
  "-DCURL_LIBRARY=$CONDA_PREFIX/lib/libcurl.so",
  "-DCURL_INCLUDE_DIR=$CONDA_PREFIX/include",
  "-DCURL_NO_CURL_CMAKE=ON",
  "-DGGML_CUDA=ON",
  "-S",
  "llama.cpp/",
  "-B",
  ".build/",
] }
build = { cmd = "cmake --build .build/", depends-on = ["configure"] }
install = { cmd = "cmake --install .build/ --prefix $CONDA_PREFIX", depends-on = [
  "build",
] }

start = { cmd = [
  "llama-server",
  "-hf",
  "ggml-org/gemma-3-12b-it-GGUF",
  "--port",
  "3030",
  "--host",
  "100.87.144.124",
] }

gguf-gui = "gguf-editor-gui"

[dependencies]
libcurl = ">=8.11.1,<9"
cuda = "12.4.1"
huggingface_hub = ">=0.26.5,<0.27"

[pypi-dependencies]
gguf = { path = "llama.cpp/gguf-py", extras = ["gui"] }
